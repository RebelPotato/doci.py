# # Doci
#
# Doci is a quick and dirty tool for generating documentation that show
# explainations and code side by side, a.k.a. "semi-
# [literate programming](https://en.wikipedia.org/wiki/Literate_programming)".
# This is very much a remix of [Pycco](https://github.com/pycco-docs/pycco),
# which itself is a Python port of [Docco](https://github.com/jashkenas/docco).
#
# [TODO: screenshot]
#
# It is quite dumb and need to rely on other tools, but I think it's better than
# being smart and trying to do everything by itself. For example, `scripts/build.py`
# watches `doci.py` for changes and reruns it automatically, which
# is a very simple way to add file watching abilities to Doci.
#
# It is also well-documented. Doci is <250 lines of code plus comments for now,
# so it should be easy to read and modify.
#
# ## Usage
#
# To generate the documentation for itself, run:
#
# ```bash
# uv run doci.py
# ```
#
# Does not support any other files yet... Working on it!
#
# ## Code
#
# This document is generated by `doci.py` when given its own source.

import re
import argparse
import pygments
from typing import TypedDict, List, Tuple
from dataclasses import dataclass

# I use snoop for print debugging.
# import snoop


#### Utility functions
def running(f, iterable, init=None):
    """
    Calculate a running value from an iterable using a binary function `f`.
    """
    for x in iterable:
        init = x if init is None else f(init, x)
        yield init


def running_sum(iterable, init=None):
    """
    Calculate a running sum.

    ``` python
    >> list(running_sum(range(5)))
    [0, 1, 3, 6, 10, 15]

    >> list(running_sum(range(5), 10))
    [10, 11, 13, 16, 20, 25]
    ```
    """
    return running(lambda x, y: x + y, iterable, init)


def indent_size(s: str) -> int:
    # A string's indent size is the number of leading spaces.
    for i, c in enumerate(s):
        if c != " ":
            return i
    return len(s)


def dedent(s: List[str]) -> List[str]:
    """
    Dedent a list of strings, removing the minimum leading whitespace
    from all visible lines. Invisible lines (empty or whitespace-only)
    are left unchanged.
    """
    stripped = [line.strip() for line in s]
    min_indent = min(
        (indent_size(line) for line, st in zip(s, stripped) if st != ""), default=None
    )
    if min_indent is None:
        return s
    return [line[min_indent:] if st != "" else line for line, st in zip(s, stripped)]


#### Extracting chunks
def parse_blockquote(
    contents: List[str], block_syms: Tuple[str, str]
) -> Tuple[List[int], List[int]]:
    """
    Parse out blockquotes from a list of contents.
    """
    is_blockquote = [
        1 if c.startswith(block_syms[0]) else -1 if c.startswith(block_syms[1]) else 0
        for c in contents
    ]
    if block_syms[0] == block_syms[1]:
        # every other blockquote is an end quote.
        blockquote_locs = [i for i, v in enumerate(is_blockquote) if v == 1]
        for i, p in enumerate(blockquote_locs):
            if i % 2 == 1:
                is_blockquote[p] = -1
    level = [0] * len(contents)
    seen_quote = 0
    for i, v in enumerate(is_blockquote):
        if v == 1:
            # We don't handle recursive blockquotes, because most languages don't support them.
            # If we do, we would need to increment `seen_quote` here.
            seen_quote = 1
        elif v == -1:
            # This works even if there are more end quotes than start quotes.
            seen_quote = max(0, seen_quote - 1)
        level[i] = seen_quote
    return is_blockquote, level


def extract_chunks(
    code: str, comment_start: str, block_syms: Tuple[str, str]
) -> Tuple[List[str], List[str]]:
    """
    Extract doc and code chunks from a program. We return a list of chunks
    and a list of booleans indicating whether each chunk is a doc chunk.
    """
    lines = code.splitlines(keepends=True)
    lc = len(lines)  # line count
    # Contents are lines stripped of whitespace and line breaks.
    contents = [line.strip() for line in lines]
    is_comment = [c.startswith(comment_start) for c in contents]
    is_blockquote, level = parse_blockquote(contents, block_syms)

    # Chunks are consecutive lines. Docs are chunks that are comments or blockquoted.
    is_doc = [
        is_comment[i] or is_blockquote[i] != 0 or level[i] != 0 for i in range(lc)
    ]
    # Chunks are always interspersed, meaning that for each two doc chunks
    # there is a code chunk in between, and vice versa.
    is_chunk_top = [i == 0 or is_doc[i - 1] != is_doc[i] for i in range(lc)]
    chunk_top_loc = [i for i, v in enumerate(is_chunk_top) if v]
    chunk_bottom_loc = chunk_top_loc[1:] + [lc]

    # doc chunks are stripped of its starting symbols and whitespaces,
    # while code chunks are left as is.
    def strip(i: int, s: str) -> str:
        if is_comment[i]:
            return contents[i].removeprefix(comment_start) + "\n"
        if is_blockquote[i] != 0:
            # We ignore all blockquote lines!
            return ""
        if is_doc[i]:
            return contents[i] + "\n"
        return s

    chunk_lines = [strip(i, s) for i, s in enumerate(lines)]
    chunk_type = ["doc" if is_doc[i] else "code" for i in chunk_top_loc]
    chunks = [
        "".join(chunk_lines[i:j]) for i, j in zip(chunk_top_loc, chunk_bottom_loc)
    ]
    return chunk_type, chunks


#### Formatting
def highlight(code_chunks: List[str], language: str, comment_start: str) -> List[str]:
    """
    Turn a list of code chunks into HTML using pygments.
    The code chunks are merged and passed pygments in one go,
    which is more efficient than highlighting each chunk separately.
    """
    # Magic divider text and html copied from pycco.
    divider_text = f"\n{comment_start}DIVIDER\n"
    divider_html = re.compile(
        rf'\n*<span class="c[1]?">{comment_start}DIVIDER</span>\n*'
    )
    lexer = pygments.lexers.get_lexer_by_name(language)

    # To do this, we join all code chunks into a single program
    # with a piece of divider text in between,
    joined_code = divider_text.join(code_chunks)
    # highlight the whole thing using pygments,
    html_formatter = pygments.formatters.get_formatter_by_name("html")
    output = (
        pygments.highlight(joined_code, lexer, html_formatter)
        .replace('<div class="highlight"><pre>', "")
        .replace("</pre></div>", "")
    )
    # then split it into html to get the html for each code section.
    return re.split(divider_html, output)


def to_markdown(chunk_type: List[str], chunks: List[str], language: str) -> str:
    """
    Convert the chunks into a markdown string.
    """
    md = []
    for i, chunk in enumerate(chunks):
        if chunk_type[i] == "doc":
            # Doc chunks are dedented.
            md.append("".join(dedent(chunk.splitlines(keepends=True))))
        else:
            # Code chunks are stripped of trailing spaces and newlines.
            stripped = chunk.rstrip()
            if stripped != "":
                md.append(f"```{language}\n{chunk.rstrip()}\n```\n")
    return "\n".join(md)


#### Command line interface
def main():
    with open("doci.py", "r") as f:
        code = f.read()
    chunk_type, chunks = extract_chunks(code, "#", ('"""', '"""'))
    # Hardcoded to generate this source to README.md file for now.
    with open("README.md", "w") as f:
        f.write(to_markdown(chunk_type, chunks, "python"))


if __name__ == "__main__":
    main()
