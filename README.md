# Literally

Literally is a quick and dirty tool for generating documentation that show
explainations and code side by side, a.k.a. "semi-
[literate programming](https://en.wikipedia.org/wiki/Literate_programming)".
This is very much a remix of [Pycco](https://github.com/pycco-docs/pycco),
which itself is a Python port of [Docco](https://github.com/jashkenas/docco).

[TODO: screenshot]

It is quite dumb and need to rely on other tools, but I think it's better than
being smart and trying to do everything by itself. For example, `scripts/build.py`
watches `literally.py` for changes and reruns it automatically, which
is a very simple way to add file watching abilities to Literally.

Literally is <200 lines of code plus comments for now, so it should be
easy to read and modify.

## Usage

To generate the documentation for itself, run:

```bash
uv run literally.py
```

Does not support any other files yet... Working on it!

## Code

This document is generated by `literally.py` when given its own source.

```python

import re
import argparse
import pygments
from typing import TypedDict, List, Tuple
from dataclasses import dataclass
```

I use snoop for print debugging.

```python
import snoop
```

### Utility functions

```python
def running(f, iterable, init=None):
```

Calculate a running value from an iterable using a binary function `f`.


```python
    for x in iterable:
        init = x if init is None else f(init, x)
        yield init


def running_sum(iterable, init=None):
```

Calculate a running sum.

``` python
>> list(running_sum(range(5)))
[0, 1, 3, 6, 10, 15]

>> list(running_sum(range(5), 10))
[10, 11, 13, 16, 20, 25]
```


```python
    return running(lambda x, y: x + y, iterable, init)


def indent_size(s: str) -> int:
```

A string's indent size is the number of leading spaces.

```python
    for i, c in enumerate(s):
        if c != " ":
            return i
    return len(s)


def dedent(s: List[str]) -> List[str]:
```

Dedent a list of strings, removing the minimum leading whitespace
from all visible lines. Invisible lines (empty or whitespace-only)
are left unchanged.


```python
    stripped = [line.strip() for line in s]
    min_indent = min(
        (indent_size(line) for line, st in zip(s, stripped) if st != ""), default=None
    )
    if min_indent is None:
        return s
    return [line[min_indent:] if st != "" else line for line, st in zip(s, stripped)]
```

### Extracting chunks

```python
def parse_blockquote(
    contents: List[str], block_syms: Tuple[str, str]
) -> Tuple[List[int], List[int]]:
```

Parse out blockquotes from a list of contents.


```python
    is_blockquote = [
        1 if c.startswith(block_syms[0]) else -1 if c.startswith(block_syms[1]) else 0
        for c in contents
    ]
```

We don't handle recursive blockquotes, because most languages don't support them.

```python
    level = [0] * len(contents)
    seen_quote = 0
    for i, v in enumerate(is_blockquote):
        if v == 1:
            level[i] = 1
            seen_quote = 1
        elif v == -1:
```

The end quote has level 1 too.

```python
            level[i] = seen_quote
            seen_quote = max(0, seen_quote - 1)
        else:
            level[i] = seen_quote
    return is_blockquote, level


def extract_chunks(
    code: str, comment_start: str, block_syms: Tuple[str, str]
) -> Tuple[List[str], List[str]]:
```

Extract doc and code chunks from a program. We return a list of chunks
and a list of booleans indicating whether each chunk is a doc chunk.


```python
    lines = code.splitlines(keepends=True)
    lc = len(lines)  # line count
```

Contents are lines stripped of whitespace and line breaks.

```python
    contents = [line.strip() for line in lines]
    is_comment = [c.startswith(comment_start) for c in contents]
    is_blockquote, level = parse_blockquote(contents, block_syms)
```

Chunks are consecutive lines. Docs are chunks that are comments or blockquoted.

```python
    is_doc = [is_comment[i] or is_blockquote[i] or level[i] != 0 for i in range(lc)]
```

Chunks are always interspersed, meaning that for each two doc chunks
there is a code chunk in between, and vice versa.

```python
    is_chunk_top = [i == 0 or is_doc[i - 1] != is_doc[i] for i in range(lc)]
    chunk_top_loc = [i for i, v in enumerate(is_chunk_top) if v]
    chunk_bottom_loc = chunk_top_loc[1:] + [lc]
```

doc chunks are stripped of its starting symbols and whitespaces,
while code chunks are left as is.

```python
    def strip(i: int, s: str) -> str:
        if is_comment[i]:
            return contents[i].removeprefix(comment_start) + "\n"
        if is_blockquote[i] != 0:
```

We ignore all blockquote lines for now!

```python
            return ""
        if is_doc[i]:
            return contents[i] + "\n"
        return s

    chunk_lines = [strip(i, s) for i, s in enumerate(lines)]
    chunk_type = ["doc" if is_doc[i] else "code" for i in chunk_top_loc]
    chunks = [
        "".join(chunk_lines[i:j]) for i, j in zip(chunk_top_loc, chunk_bottom_loc)
    ]
    return chunk_type, chunks
```

### Formatting

```python
def highlight(code_chunks: List[str], language: str, comment_start: str) -> List[str]:
```

Turn a list of code chunks into HTML using pygments.
The code chunks are merged and passed pygments in one go,
which is more efficient than highlighting each chunk separately.


Magic divider text and html copied from pycco.

```python
    divider_text = f"\n{comment_start}DIVIDER\n"
    divider_html = re.compile(
        rf'\n*<span class="c[1]?">{comment_start}DIVIDER</span>\n*'
    )
    lexer = pygments.lexers.get_lexer_by_name(language)
```

To do this, we join all code chunks into a single program
with a piece of divider text in between,

```python
    joined_code = divider_text.join(code_chunks)
```

highlight the whole thing using pygments,

```python
    html_formatter = pygments.formatters.get_formatter_by_name("html")
    output = (
        pygments.highlight(joined_code, lexer, html_formatter)
        .replace('<div class="highlight"><pre>', "")
        .replace("</pre></div>", "")
    )
```

then split it into html to get the html for each code section.

```python
    return re.split(divider_html, output)


def to_markdown(chunk_type: List[str], chunks: List[str], language: str) -> str:
```

Convert the chunks into a markdown string.


```python
    md = []
    for i, chunk in enumerate(chunks):
        if chunk_type[i] == "doc":
```

Doc chunks are dedented.

```python
            md.append("".join(dedent(chunk.splitlines(keepends=True))))
        else:
```

Code chunks are stripped of trailing spaces and newlines.

```python
            md.append(f"```{language}\n{chunk.rstrip()}\n```\n")
    return "\n".join(md)
```

### Command line interface

```python
def main():
    with open("literally.py", "r") as f:
        code = f.read()
```

*important hack*: we use four double quotes to denote docstrings
that we want to extract as documentation.

```python
    chunk_type, chunks = extract_chunks(code, "#", ('""" "', '" """'))
```

Hardcoded to generate this source to README.md file for now.

```python
    with open("README.md", "w") as f:
        f.write(to_markdown(chunk_type, chunks, "python"))


if __name__ == "__main__":
    main()
```
